---
title: "Provenance and Reproducibility"
format: 
  revealjs:
    theme: simple
editor: visual
editor_options: 
  markdown: 
    wrap: sentence
---

## Learning Objectives

-   Discuss the concept of reproducible workflows including computational reproducibility and provenance metadata
-   Learn how to use R to package your work by building a reproducible paper in RMarkdown
-   Introduce tools and techniques for reproducibility supported by the NCEAS and DataONE

## This week we..

-   Introduce the concept of literate analysis using tools like Quarto
-   Discussed about Data Management, metadata, acceding and publishing data
-   Talked about data modeling essentials
-   Discussed the FAIR and CARE principles
-   Introduced data wrangling functions
-   Learned about functions and packages in R

On top of this..
You stared framing your synthesis projects

## Reproducible Research: Recap

Working in a reproducible manner:

-   Increases research efficiency, accelerating the pace of your research and collaborations.
-   Provides transparency by capturing and communicating scientific workflows.
-   Enables research to stand on the shoulders of giants (build on work that came before).
-   Allows credit for secondary usage and supports easy attribution.
-   Increases trust in science.

## Smith et al

![](images/Smith-et-al.png)

The **methods sections of papers are typically inadequate to fully reproduce the work described in the paper**.

::: notes
To enable others to fully interpret, reproduce or build upon our research, we need to provide more comprehensive information than is typically included in a figure or publication.

For example, if we look at the figure above convey multiple messages.
But, by looking at the figure we don't get the full story how did scientist got to make this plot.
What data were used in this study?
What methods applied?
What were the parameter settings?
What documentation or code are available to us to evaluate the results?
Can we trust these data and methods?
Are the results reproducible?
:::

## Computational reproducibility

> Is the ability to document data, analyses, and models sufficiently for other researchers to be able to understand and ideally re-execute the computations that led to scientific results and conclusions.

Practically speaking, reproducibility includes:

-   Preserving the data
-   Preserving the software workflow
-   Documenting what you did
-   Describing how to interpret it all

## Trisovic et al

![](images/code-reproducibility-trisovic.png)

::: notes
A recent study of publicly-available datasets in the Harvard Database repository containing R files found that only 26% of R files ran without error in the initial execution.
44% were able to be run after code cleaning, showing the importance of good programming practice [@trisovic2022].
The figure below from Trisovic et al. shows a *sankey diagram* of how code cleaning was able to fix common errors.
:::

## Computational Provenance and Workflows

Computational provenance refers to the origin and processing history of data including:

-   Input data
-   Workflow/scripts
-   Output data
-   Figures
-   Methods, dataflow, and dependencies

**Computational provenance is a formalized description of a workflow from the origin of the data to it's final outcome**.

::: notes
When we put these all together with formal documentation, we create a **computational workflow** that captures all of the steps from initial data cleaning and integration, through analysis, modeling, and visualization.
In other words,
:::

## Example

![](images/comp-workflow-1.png)

![](images/provenance.png)

::: notes
Here's an example of a computational workflow from Mark Carls: [Mark Carls. Analysis of hydrocarbons following the Exxon Valdez oil spill, Gulf of Alaska, 1989 - 2014. Gulf of Alaska Data Portal. urn:uuid:3249ada0-afe3-4dd6-875e-0f7928a4c171.](https://search.dataone.org/view/urn%3Auuid%3A3249ada0-afe3-4dd6-875e-0f7928a4c171), that represents a three step workflow comprising four source data files and two output visualizations.

This image is a screenshot of an interactive user interface of a workflow built by DataONE.
You can clearly see which data files were inputs to the process, the scripts that are used to process and visualize the data, and the final output objects that are produced, in this case two graphical maps of Prince William Sound in Alaska.
:::

## From Provenance to Reproducibility

![](images/Prov-History.png)

::: notes
DataONE provides a tool to track and visualize provenance.
It facilitates reproducible science through provenance by:

-   Tracking **data derivation** history
-   Tracking data **input**s and **outputs** of analyses
-   Preserving and documenting software **workflows**
-   Tracking analysis and model **executions**
-   Linking all of these to **publications**
:::

## Data Package

One way to illustrate this is to look into the structure of a data package.

![](images/data-package.png)

::: notes
A **data package** is the unit of publication of your data, including datasets, metadata, software and provenance.
The image below represents a data package and all it's components and how these components relate to each other.
:::

## Data Citation and Transitive Credit

We want to move towards a model such that when a user cites a research publication we will also know:

-   **Which data** produced it
-   **What software** produced it
-   What was **derived** from it
-   **Who to credit** down the attribution stack

## Data Citation and Transitive Credit

Example

![](images/Transitive-Credit.png)

## Reproducible papers

A great overview of this approach to reproducible papers comes from:

> Ben Marwick, Carl Boettiger & Lincoln Mullen (2018) **Packaging Data Analytical Work Reproducibly Using R (and Friends)**, The American Statistician, 72:1, 80-88, [doi:10.1080/00031305.2017.1375986](https://doi.org/10.1080/00031305.2017.1375986)

## Reproducible Papers with `rrtools`

The key idea in Marwick et al. (2018) is that of the *research compendium*: A single container for not just the journal article associated with your research but also the underlying analysis, data, and even the required software environment required to reproduce your work.

## Research compendium

Research compendium makes it easy for researchers to do their work but also for others to inspect or even reproduce the work because all necessary materials are readily at hand due to being kept in one place.
Rather than a constrained set of rules, the research compendium is a scaffold upon which to conduct reproducible research using open science tools such as:

-   [R](https://www.r-project.org/)
-   [RMarkdown](https://rmarkdown.rstudio.com/)
-   [Quarto](https://quarto.org/)
-   [git](https://git-scm.com/) and [GitHub](https://github.com)

# Go to RStudio

## Workflow in a nutshell

::: {.callout-caution icon="false"}
## Summary

-   Use `rrtools` to generate the core directory layout and approach to data handling.
-   Then use `rticles` to create the structure of the paper itself. The combination is incredibly flexible.
:::

## Things we can do with our research compendium:

-   Edit `./analysis/paper/paper.Rmd` to begin writing your paper and your analysis in the same document
-   Add any citations to `./analysis/paper/pnas-sample.bib`
-   Add any longer R scripts that don't fit in your paper in an `R` folder at the top level
-   Add raw data to `./data/raw_data`
-   Write out any derived data (generated in `paper.Rmd`) to `./data/derived_data`
-   Write out any figures in `./analysis/figures`

::: notes
You can then write all of your R code in your RMarkdown, and generate your manuscript all in the format needed for your journal (using it's .csl file, stored in the paper directory).
:::

## Adding `renv` to conserve your environment

-   `rrtools` has a couple more tricks up it's sleeve to help your compendium be as reproducible and portable as possible.

-   To capture the R packages and versions this project depends on, we can use the `renv` package.

Let's see how it works...

::: notes
-   Running `renv::init()`, will initiate tracking of the R packages in your project.

-   This action will create a new folder called `renv` in your top directory.

-   `renv::init()` automatically detects dependencies in your code (by looking for library calls, at the DESCRIPTION file, etc.) and installs them to a private project specific library.
    This means that your project `mypaper` can use a different version of `dplyr` than another project which may need an older version without any hassle.

-   `renv` also write the package dependencies to a special file in the repository called `renv.lock`.

-   If any of your packages you are using is updated, while your are working on your project, you can run `renv::snapshot()` to update the `renv.lock` file and your project-installed packages.

-   You can read the `renv.lock` file using `renv::restore()`, when needed.
    This will install the versions of the packages needed.
:::

## Conserve your computational environement with Docker

-   The `rrtools` package then uses this `renv.lock` file to build what is called a Dockerfile.
-   [**Docker**](http://www.docker.com) **allows you to build containers, a standard unit of software that packages up code and all its dependencies so an application runs quickly and reliably from one computing environment to another.**

::: notes
-   A container is an "image" of all the software specified, and this image can be run on other computers such that the software stack looks exactly as you specify.
-   This is important when it comes to reproducibility, because when running someone else code, you may get different results or errors if you are using different versions of software (like an old version of `dplyr`).
-   A Dockerfile contains the instructions for how to recreate the computational environment where your analysis was run.
:::

## In practice

-   Once you have your research compendium, you can called `rrtools::use_dockerfile()`

-   This, first creates a Dockerfile that loads a standard image for using R with the tidyverse,

-   And then has more instructions for how to create the environment so that it has the very specific R packages and versions you need.

-   If we look at the Dockerfile (example below), it calls to `renv::restore()`, as described above.

-   The last line of the docker file renders our RMarkdown reproducible paper!

## GitHub Actions

-   After running `rrtools::use_dockerfile()`, the package also sets up GitHub Actions for you.
-   Actions are processes that are triggered in GitHub events (like a push) and run automatically.
-   In this case, the Action that is set up will build your Docker image on GitHub.
-   This means that the code that knits your paper is run, and an updated version of your paper is knit.
-   This is called **continuous integration,** and is extremely convenient for developing products like this, since the build step can be taken care of automatically as you push to your repository.

![](images/github-actions.png)

## The 5th Generation of Reproducible Papers

![](images/Living-paper.png)

## Whole Tale

Tales are executable research objects captured in a standards-based tale format complete with metadata.
They can contain:

![](images/whole-tale-container.png)

::: notes
By combining data, code and the compute environment, tales allow researchers to:

-   Re-create the computational results from a scientific study
-   Achieve computational reproducibility
-   "Set the default to reproducible."
:::

## Resources

-   [`rrtools` documentation](https://github.com/benmarwick/rrtools)
-   The [`rticles`](https://github.com/rstudio/rticles%20B)
-   [`usethis` documentation](https://usethis.r-lib.org/)
